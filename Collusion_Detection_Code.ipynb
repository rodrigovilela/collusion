{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f36fcd-143a-4b47-b76e-a843eeb95478",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from joblib import dump, load\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, balanced_accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from collections import defaultdict\n",
    "from scipy import stats\n",
    "from itertools import cycle\n",
    "\n",
    "\n",
    "# Font size to plot\n",
    "default_font_size = 18\n",
    "plt.rcParams.update({'font.size': default_font_size})\n",
    "\n",
    "# Format to print\n",
    "pd.options.display.float_format = '{:,.4f}'.format\n",
    "\n",
    "# To hide warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Paths and filenames of the datasets\n",
    "path = os.path.abspath('') #os.path.dirname(os.path.abspath(__file__))\n",
    "db_collusion_brazilian_comprasnet = os.path.join(path, 'DB_Collusion_Brazil_Comprasnet_processed.csv')\n",
    "#db_collusion_brazilian_comprasnet = os.path.join(path, 'DB_Collusion_Brazil_Comprasnet_regras_processed.csv')\n",
    "db_collusion_brazilian = os.path.join(path, 'DB_Collusion_Brazil_processed.csv')\n",
    "db_collusion_italian = os.path.join(path, 'DB_Collusion_Italy_processed.csv')\n",
    "db_collusion_american = os.path.join(path, 'DB_Collusion_America_processed.csv')\n",
    "db_collusion_switzerland_gr_sg = os.path.join(path, 'DB_Collusion_Switzerland_GR_and_See-Gaster_processed.csv')\n",
    "db_collusion_switzerland_ticino = os.path.join(path, 'DB_Collusion_Switzerland_Ticino_processed.csv')\n",
    "db_collusion_japan = os.path.join(path, 'DB_Collusion_Japan_processed.csv')\n",
    "db_collusion_all = os.path.join(path, 'DB_Collusion_All_processed.csv')\n",
    "\n",
    "# To save plots (pdf format)\n",
    "plot_pdf = True \n",
    "\n",
    "# User's parameters for the functions\n",
    "ml_algorithms = ['GaussianProcessClassifier', 'SGDClassifier', 'ExtraTreesClassifier', 'RandomForestClassifier', 'AdaBoostClassifier', \n",
    "                  'GradientBoostingClassifier', 'SVC', 'KNeighborsClassifier', 'MLPClassifier', 'BernoulliNB', 'GaussianNB'] \n",
    "screens = ['CV', 'SPD', 'DIFFP', 'RD', 'KURT', 'SKEW', 'KSTEST'] # Screening variables to use. There are seven: CV, SPD, DIFFP, RD, KURT, SKEW and KSTEST \n",
    "train_size = 0.8 # Test and train sizes. The test_size is 1-train_size\n",
    "repetitions = 50 # Number of repetitions for each ML algorithm. Minimum value > 30. Recommended value > 100\n",
    "n_estimators = 300 # Number of estimators for ML algorithms\n",
    "precision_recall = True # To plot precision-recall curves\n",
    "load_data = False # To load the error metrics (to load previous data experimentation)\n",
    "save_data = True # To save the error metrics (to persist the data experimentation)\n",
    "\n",
    "\n",
    "def shuffle_tenders(df1):\n",
    "    ''' Shuffle tenders. The reason is that maybe the colluded tenders are concentrated in some parts of the excel (dataframe)'''\n",
    "\n",
    "    df = df1.copy()\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    df['Tender'] = df['Tender'].astype(str)\n",
    "    reindex_tenders = 1\n",
    "    list_tenders = []\n",
    "    for index, row in df.iterrows():\n",
    "        if not row['Tender'] in list_tenders:\n",
    "            df['Tender'].replace(row['Tender'], reindex_tenders, inplace=True)\n",
    "            reindex_tenders = reindex_tenders + 1\n",
    "            list_tenders.append(row['Tender'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_colluded_tenders_by_bidder(df):\n",
    "    ''' Calculate the colluded tenders by bidder and print the results '''\n",
    "\n",
    "    df_aux = df.copy()\n",
    "    df_tenders_by_bidder = df_aux.groupby(['Competitors']).size().reset_index(name='Total_tenders')\n",
    "    df_aux['Collusive_competitor'] = df_aux['Collusive_competitor'].apply(lambda x: 1 if x > 0 else x)\n",
    "    df_tenders_by_bidder['Total_colluded_tenders'] = df_aux.groupby(['Competitors'])['Collusive_competitor'].sum()\n",
    "    df_tenders_by_bidder['Ratio'] = df_tenders_by_bidder['Total_colluded_tenders'] / df_tenders_by_bidder['Total_tenders'] * 100\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "        print(df_tenders_by_bidder)\n",
    "\n",
    "\n",
    "def printScatterMatrix(df, color_labels, colors, labels_legend, dataset):\n",
    "    ''' Scatter matrix for a dataframe '''\n",
    "\n",
    "    plt.rcParams.update({'font.size': 11}) # Font size to plot\n",
    "    sm = pd.plotting.scatter_matrix(df, figsize=(12, 16), diagonal='kde', alpha=0.35, color=color_labels, s=3, rasterized=True) # kde or hist\n",
    "    n = len(df.columns)\n",
    "    for x in range(n):\n",
    "        for y in range(n):\n",
    "            # to get the axis of subplots\n",
    "            ax = sm[x, y]\n",
    "            # to make x axis name vertical  \n",
    "            #ax.xaxis.label.set_rotation(330)\n",
    "            # to make y axis name horizontal \n",
    "            #ax.yaxis.label.set_rotation(0)\n",
    "            # to make sure y axis names are outside the plot area\n",
    "            #ax.yaxis.labelpad = 40\n",
    "            ax.xaxis.labelpad = 20\n",
    "            # to show the half of the scatter matrix\n",
    "            if x < y:\n",
    "                sm[x, y].set_visible(False)\n",
    "            # adjust xlim, ylim\n",
    "            if x == 1:\n",
    "                ax.set_ylim([0, 0.4])\n",
    "            elif x == 2:\n",
    "                ax.set_ylim([0, 0.5])\n",
    "            elif x == 3:\n",
    "                ax.set_ylim([0, 0.4])\n",
    "            elif x == 4:\n",
    "                ax.set_ylim([-750, 750])\n",
    "            elif x == 5:\n",
    "                ax.set_ylim([-5, 10])\n",
    "            elif x == 6:\n",
    "                ax.set_ylim([-3, 3])\n",
    "            if y == 0:\n",
    "                ax.set_xlim([0, 0.2 * 1000000000])\n",
    "            elif y == 1:\n",
    "                ax.set_xlim([0, 0.4])\n",
    "            elif y == 2:\n",
    "                ax.set_xlim([0, 0.5])\n",
    "            elif y == 3:\n",
    "                ax.set_xlim([0, 0.4])\n",
    "            elif y == 4:\n",
    "                ax.set_xlim([-750, 750])\n",
    "            elif y == 5:\n",
    "                ax.set_xlim([-5, 10])    \n",
    "            elif y == 6:\n",
    "                ax.set_xlim([-3, 3])\n",
    "    # More bottom margin to read x and y axis\n",
    "    plt.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "    # Legend\n",
    "    handles = [plt.plot([], [], color=colors[i], ls='', marker='.', markersize=np.sqrt(90))[0] for i in range(len(colors))]\n",
    "    plt.legend(handles, labels_legend, loc=(-1,4)) \n",
    "    # Draw\n",
    "    plt.draw()\n",
    "    if plot_pdf:\n",
    "        name_file = dataset + '_Scatter_matrix.pdf'\n",
    "        plt.savefig(name_file, format='pdf', dpi=1200, bbox_inches='tight')\n",
    "        print('Generated and saved file called ' + name_file)\n",
    "    plt.rcParams.update({'font.size': default_font_size}) # Font size to plot\n",
    "\n",
    "\n",
    "def print_boxplot(df, dataset, column_names, groupby, min_ylim, max_ylim, step_y, xlabel, percentage=True):\n",
    "    plt.rcParams.update({'font.size': 10})\n",
    "    fig = plt.figure(figsize=(2*0.7,6*0.7))\n",
    "    ax = fig.gca()\n",
    "    df.boxplot(column=column_names, by=groupby, ax=ax, fontsize=None, rot=0, grid=False, notch=True, widths=0.3, positions = (0.3, 0.9),\n",
    "                            layout=None, return_type=None, showfliers=False, meanline=True, showmeans=True, patch_artist=True, vert=True,\n",
    "                            medianprops=dict(linestyle='-', linewidth=3, color='limegreen'), meanprops=dict(linestyle='-', linewidth=3, color='firebrick'))\n",
    "    # Configurate plotting\n",
    "    ax.set_title(column_names.replace('_', ' '), fontweight='bold')\n",
    "    fig.suptitle('')\n",
    "    plt.xlabel(xlabel)\n",
    "    ax.set_ylim([min_ylim, max_ylim])\n",
    "    ax.set_yticks(np.arange(min_ylim, max_ylim+step_y, step_y))\n",
    "    ax.yaxis.grid(True, linestyle='--', alpha=0.5)\n",
    "    if percentage:\n",
    "        ax.set_yticklabels(['{:.2f}%'.format(x) for x in plt.gca().get_yticks()]) # Percentage format \n",
    "    custom_lines = [Line2D([0], [0], color='firebrick', lw=3),\n",
    "                    Line2D([0], [0], color='limegreen', lw=3)]\n",
    "    names_lines = ['Mean', 'Median']\n",
    "    ax.legend(custom_lines, names_lines, loc='center', bbox_to_anchor=(1.75, 0.125))\n",
    "    plt.draw()\n",
    "    if plot_pdf:\n",
    "        name_file = dataset + '_BoxPlot_' + column_names + '.pdf'\n",
    "        fig.savefig(name_file, format='pdf', dpi=1200, bbox_inches='tight')\n",
    "        print('Generated and saved file called ' + name_file)\n",
    "    plt.rcParams.update({'font.size': default_font_size})\n",
    "\n",
    "\n",
    "def predict_collusion_company(df, dataset, predictors_column_name, targets_column_name, algorithm, train_size, n_estimators=None):\n",
    "    ''' Predict collusion applying the ML algorithm '''\n",
    "\n",
    "    # Datasets to have to simplify the process' time \n",
    "    simplify_process = ['japan', 'italian', 'switzerland_gr_sg', 'american', 'all']\n",
    "\n",
    "    # To assing the dataframes\n",
    "    predictors = df[predictors_column_name]\n",
    "    targets = df[targets_column_name]\n",
    "\n",
    "    # We create the training and test sample, both for predictors and for the objective variable, based on the tender group. \n",
    "    # That is, the bids of a tender either all own to the train group or the test group. They cannot be divided between both groups. \n",
    "    gss = GroupShuffleSplit(n_splits=5, train_size=train_size)\n",
    "    train_index, test_index = next(gss.split(predictors, targets, groups=df['Tender']))\n",
    "    x_train = predictors.loc[train_index]\n",
    "    y_train = targets.loc[train_index]\n",
    "    x_test = predictors.loc[test_index]\n",
    "    y_test = targets.loc[test_index]\n",
    "\n",
    "    # Train the model with the selected algorithm\n",
    "    if algorithm == 'ExtraTreesClassifier':\n",
    "        classifier = ExtraTreesClassifier(n_estimators=n_estimators, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n",
    "                            max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, \n",
    "                            oob_score=True, n_jobs=-1, random_state=None, verbose=0, warm_start=False, class_weight='balanced', ccp_alpha=0.0, max_samples=None)\n",
    "    elif algorithm == 'RandomForestClassifier':\n",
    "        classifier = RandomForestClassifier(n_estimators=n_estimators, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0., \n",
    "                            max_features=None, max_leaf_nodes=None, min_impurity_decrease=0., min_impurity_split=None, bootstrap=True, \n",
    "                            oob_score=True, n_jobs=-1, random_state=None, verbose=0, warm_start=False, class_weight='balanced')\n",
    "    elif algorithm == 'SGDClassifier':\n",
    "        classifier = SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=10000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, \n",
    "                            n_jobs=-1, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, \n",
    "                            class_weight=None, warm_start=False, average=False)\n",
    "    elif algorithm == 'AdaBoostClassifier':\n",
    "        classifier = AdaBoostClassifier(base_estimator=None, n_estimators=n_estimators, learning_rate=1.0, algorithm='SAMME.R', random_state=None)\n",
    "    elif algorithm == 'GradientBoostingClassifier':\n",
    "        if dataset in simplify_process:\n",
    "            learning_rate = 100\n",
    "            tol = 10\n",
    "            estimators = int(round(n_estimators / 3))\n",
    "        else:\n",
    "            learning_rate = 0.1\n",
    "            tol = 0.0001\n",
    "            estimators = n_estimators\n",
    "        classifier = GradientBoostingClassifier(loss='deviance', learning_rate=learning_rate, n_estimators=estimators, subsample=1.0, criterion='mae', min_samples_split=2, min_samples_leaf=1, \n",
    "                            min_weight_fraction_leaf=0.0, max_depth=None, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0, \n",
    "                            max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=tol, ccp_alpha=0.0)\n",
    "    elif algorithm == 'SVC':\n",
    "        classifier = SVC(C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, \n",
    "                            class_weight='balanced', verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)\n",
    "    elif algorithm == 'KNeighborsClassifier':\n",
    "        classifier = KNeighborsClassifier(n_neighbors=10, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=-1)\n",
    "    elif algorithm == 'MLPClassifier':\n",
    "        classifier = MLPClassifier(hidden_layer_sizes=(240, 120, 70, 35), activation='logistic', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, \n",
    "                        power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=0, warm_start=False, momentum=0.9, nesterovs_momentum=True, \n",
    "                        early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)\n",
    "    elif algorithm == 'GaussianNB':\n",
    "        classifier = GaussianNB(priors=None, var_smoothing=1e-09)\n",
    "    elif algorithm == 'BernoulliNB':\n",
    "        classifier = BernoulliNB(alpha=0.5, binarize=0, fit_prior=True, class_prior=None)\n",
    "    elif algorithm == 'GaussianProcessClassifier':\n",
    "        if dataset in simplify_process:\n",
    "            max_iter_predict = 5\n",
    "            n_restarts_optimizer = 2\n",
    "        else:\n",
    "            max_iter_predict = 5000\n",
    "            n_restarts_optimizer = 50\n",
    "        classifier = GaussianProcessClassifier(kernel=None, optimizer='fmin_l_bfgs_b', n_restarts_optimizer=n_restarts_optimizer, max_iter_predict=max_iter_predict, warm_start=False, copy_X_train=True, random_state=None, \n",
    "                        multi_class='one_vs_rest', n_jobs=-1)\n",
    "    \n",
    "    # We build the model for the train group\n",
    "    classifier = classifier.fit(x_train, y_train.values.ravel())\n",
    "  \n",
    "    # We predict for the values of the test group\n",
    "    predictions = classifier.predict(x_test)\n",
    "    df_predictions = pd.DataFrame(data=predictions, index=y_test.index, columns=['Forecast_collusive_competitor'])\n",
    "    \n",
    "    # To calculate the error metrics for the classification binary model\n",
    "    accuracy = accuracy_score(y_test, predictions) * 100\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, predictions) * 100\n",
    "    precision = precision_score(y_test, predictions, pos_label=1, average='binary', zero_division=1) * 100 # Ratio of true positives: tp / (tp + fp)\n",
    "    recall = recall_score(y_test, predictions, pos_label=1, average='binary', zero_division=1) * 100 # Ratio of true positives: tp / (tp + fn)\n",
    "    f1 = f1_score(y_test, predictions, pos_label=1, average='binary', zero_division=1) * 100 # F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    confusion = confusion_matrix(y_test, predictions, normalize='all') * 100\n",
    "    \n",
    "    return accuracy, balanced_accuracy, precision, recall, f1, confusion, y_test, df_predictions\n",
    "\n",
    "\n",
    "def algorithm_comparison(df, dataset, predictors, targets, algorithms, train_size, repetitions, n_estimators, precision_recall=False, load_data=False, save_data=False):\n",
    "    ''' Print table to compare Machine Learning algorithms '''\n",
    "\n",
    "    df = shuffle_tenders(df)\n",
    "    \n",
    "    for setting in predictors:\n",
    "        print('')\n",
    "        print('Generating models for ' + setting)\n",
    "        accuracy = defaultdict(list)\n",
    "        balanced_accuracy = defaultdict(list)\n",
    "        false_positive = defaultdict(list)\n",
    "        false_negative = defaultdict(list)\n",
    "        precision = defaultdict(list)\n",
    "        recall = defaultdict(list)\n",
    "        f1 = defaultdict(list)\n",
    "        tenders_test = defaultdict(list)\n",
    "        tenders_predictions = defaultdict(list)\n",
    "\n",
    "        # Create namefile\n",
    "        namefile = dataset + '_ML_algorithms_experimentation_' + setting + '_' + str(repetitions) + 'repetitions'\n",
    "        \n",
    "        if load_data == False:\n",
    "            for algorithm in algorithms:\n",
    "                print('Training algorithm ' + algorithm)\n",
    "                df_copy = df.copy()\n",
    "                if algorithm in ['GaussianProcessClassifier', 'GradientBoostingClassifier', 'SVC']:\n",
    "                    loop = int(round(repetitions / 40))\n",
    "                    if dataset == 'all' and algorithm == 'GaussianProcessClassifier':\n",
    "                        # Exception: reduce the datataset to be able to compute this dataset and algorithm\n",
    "                        df_copy = df_copy.sample(frac=0.5).reset_index(drop=True)\n",
    "                else:\n",
    "                    loop = repetitions\n",
    "                for i in range(loop):\n",
    "                    item_accuracy, item_balanced_accuracy, item_precision, item_recall, item_f1, confusion_matrix, item_tenders_test, item_tenders_predictions = \\\n",
    "                                predict_collusion_company(df_copy, dataset, predictors[setting], targets, algorithm, train_size, n_estimators)\n",
    "                    accuracy[algorithm].append(item_accuracy)\n",
    "                    balanced_accuracy[algorithm].append(item_balanced_accuracy)\n",
    "                    if confusion_matrix.shape[1] == 2:\n",
    "                        false_positive[algorithm].append(confusion_matrix[0][1])\n",
    "                        false_negative[algorithm].append(confusion_matrix[1][0])\n",
    "                    else:\n",
    "                        false_positive[algorithm].append(0)\n",
    "                        false_negative[algorithm].append(0)\n",
    "                    precision[algorithm].append(item_precision)\n",
    "                    recall[algorithm].append(item_recall)\n",
    "                    f1[algorithm].append(item_f1)\n",
    "                    tenders_test[algorithm].append(item_tenders_test)\n",
    "                    tenders_predictions[algorithm].append(item_tenders_predictions)\n",
    "\n",
    "            # Save dictionaries to persist the data experimentation \n",
    "            if save_data:\n",
    "                path_namefile = os.path.join(path, namefile + '.pkl')\n",
    "                file = [accuracy, balanced_accuracy, false_positive, false_negative, precision, recall, f1, df, tenders_test, tenders_predictions]\n",
    "                dump(file, path_namefile, compress=6)\n",
    "               \n",
    "        else:\n",
    "            # To load data\n",
    "            pkl_file = os.path.join(path, namefile + '.pkl')\n",
    "            [accuracy, balanced_accuracy, false_positive, false_negative, precision, recall, f1, df, tenders_test, tenders_predictions] = load(pkl_file)\n",
    "        \n",
    "        for algorithm in algorithms:\n",
    "            # Print error metrics\n",
    "            test_size = 1 - train_size\n",
    "            print('Algorithm {} with train:test {:,.2f}:{:,.2f}, {} repetitions and {}: mean_accuracy={:,.1f}, mean_FP={:,.1f}, ' \n",
    "                'mean_FN={:,.1f}, mean_balanced_accuracy={:,.1f}, mean_f1={:,.1f}, median_f1={:,.1f}, mean_precision={:,.1f}, '\n",
    "                'median_precision={:,.1f}, mean_recall={:,.1f} and median_recall={:,.1f}'.format(\n",
    "                algorithm, train_size, test_size, repetitions, setting, np.mean(accuracy[algorithm]), np.mean(false_positive[algorithm]), np.mean(false_negative[algorithm]), \n",
    "                np.mean(balanced_accuracy[algorithm]), np.mean(f1[algorithm]), np.median(f1[algorithm]), np.mean(precision[algorithm]),\n",
    "                np.median(precision[algorithm]), np.mean(recall[algorithm]), np.median(recall[algorithm])))\n",
    "        \n",
    "        # Print curve precision vs recall with iso-F1 lines\n",
    "        if precision_recall:\n",
    "            #plot_precision_vs_recall(dataset, algorithms, precision, recall, min_f1=0.4, max_f1=0.86, f1_curves=24, min_x_y_lim=0.5, max_x_y_lim=1, namefile=namefile)\n",
    "            plot_precision_vs_recall(dataset, algorithms, precision, recall, min_f1=0.05, max_f1=0.86, f1_curves=24, min_x_y_lim=0.05, max_x_y_lim=1, namefile=namefile)\n",
    "\n",
    "\n",
    "def plot_precision_vs_recall(dataset, algorithms, precision, recall, min_f1, max_f1, f1_curves, min_x_y_lim, max_x_y_lim, namefile=None):\n",
    "    ''' Plot the precision vs recall with F1 Score iso-curves to compare the ML algorithms. \n",
    "        The point to cut both lines (precision and recall) is median of the F1 score. \n",
    "        This is necessary to correspond the point with the F1 Score iso-curves'''\n",
    "\n",
    "    plt.rcParams.update({'font.size': 26}) # Font size to plot\n",
    "\n",
    "    # Colors and markers for the plot to compare 11 algorithms\n",
    "    colors = cycle(['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown',\n",
    "                    'tab:pink', 'tab:gray', 'tab:olive', 'tab:cyan', 'gold'])\n",
    "    markers = cycle(['o', '.', 'v', '^', '<', '>', 's', 'p', 'D', 'P', 'X'])\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    f1_scores = np.linspace(min_f1, max_f1, num=f1_curves)\n",
    "    lines = []\n",
    "    labels = []\n",
    "\n",
    "    # Create iso-F1 curves\n",
    "    for f1_scores in f1_scores:\n",
    "        x = np.linspace(0.01, 1)\n",
    "        y = f1_scores * x / (2 * x - f1_scores)\n",
    "        l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.65, linestyle='-.', lw=2)\n",
    "        plt.annotate('F1={0:0.2f}'.format(f1_scores), xy=(0.94, y[46] - 0.001), fontsize=17)\n",
    "    lines.append(l)\n",
    "    labels.append('F1 curves')\n",
    "\n",
    "    # Convert to [0, 1]\n",
    "    for item in algorithms:\n",
    "        recall[item] =  [x / 100 for x in recall[item]]\n",
    "        precision[item] =  [x / 100 for x in precision[item]]\n",
    "\n",
    "    # Calculate the points to plot the two lines\n",
    "    line_precision_x = defaultdict(list)\n",
    "    line_precision_y = defaultdict(list)\n",
    "    line_recall_x = defaultdict(list)\n",
    "    line_recall_y = defaultdict(list)\n",
    "    for item in algorithms:\n",
    "        line_recall_x[item] = [np.percentile(recall[item], 25), np.percentile(recall[item], 75)]\n",
    "        line_recall_y[item] = [np.median(precision[item]), np.median(precision[item])]\n",
    "        line_precision_x[item] = [np.median(recall[item]), np.median(recall[item])]\n",
    "        line_precision_y[item] = [np.percentile(precision[item], 25), np.percentile(precision[item], 75)]\n",
    "    \n",
    "    # Plot the two lines and the point to cut both lines\n",
    "    for item, color in zip(algorithms, colors): # It can possible to use markers list\n",
    "        l, = plt.plot(line_precision_x[item], line_precision_y[item], color=color, lw=4, marker='_', markersize=14, markeredgewidth=4)\n",
    "        l, = plt.plot(line_recall_x[item], line_recall_y[item], color=color, lw=4, marker='|', markersize=14, markeredgewidth=4)\n",
    "        l, = plt.plot(np.median(recall[item]), np.median(precision[item]), color=color, markersize=8, marker='o')\n",
    "        lines.append(l)\n",
    "        labels.append('{}'.format(item))\n",
    "\n",
    "    plt.xlim([min_x_y_lim, max_x_y_lim])\n",
    "    plt.ylim([min_x_y_lim, max_x_y_lim])\n",
    "    plt.grid(dashes=(5, 10), linewidth=1)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend(lines, labels, loc='lower center', bbox_to_anchor=(0.5, -0.3), prop=dict(size=default_font_size - 2), ncol=3)\n",
    "\n",
    "    # Axis in percentage format\n",
    "    ax = fig.gca()\n",
    "    ax.set_xticklabels(['{:.0f}%'.format(x*100) for x in plt.gca().get_xticks()]) \n",
    "    ax.set_yticklabels(['{:.0f}%'.format(x*100) for x in plt.gca().get_yticks()])\n",
    "    \n",
    "    plt.draw()\n",
    "    if plot_pdf:\n",
    "        name_file = dataset + '_Precision_Recall_' + namefile + '.pdf'\n",
    "        fig.savefig(name_file, format='pdf', dpi=1200, bbox_inches='tight')\n",
    "        print('Generated and saved file called ' + name_file)\n",
    "\n",
    "\n",
    "def plotTwoHistograms(data_1, data_2, label_1, label_2, max_range, bins, max_xlim, density=True):\n",
    "    ''' Plot two histograms or density functions '''\n",
    "\n",
    "    # Fit lognormal distribution\n",
    "    data_1 = sorted(data_1.values)\n",
    "    data_2 = sorted(data_2.values)\n",
    "    shape, loc, scale = stats.lognorm.fit(data_1, loc=0)\n",
    "    data_1_prob_density_function_lognorm = stats.lognorm.pdf(data_1, shape, loc, scale)\n",
    "    shape, loc, scale = stats.lognorm.fit(data_2, loc=0)\n",
    "    data_2_prob_density_function_lognorm = stats.lognorm.pdf(data_2, shape, loc, scale)\n",
    "\n",
    "    # Plot histograms and density distributions\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    plt.hist(data_1, bins=bins, range=(0, max_range), alpha=0.3, label='Histogram: ' + label_1, facecolor='g', density=density)\n",
    "    plt.hist(data_2, bins=bins, range=(0, max_range), alpha=0.3, label='Histogram: ' + label_2, facecolor='r', density=density)\n",
    "    plt.plot(data_1, data_1_prob_density_function_lognorm, label='Probability density function (log normal): ' + label_1, color='g', linewidth=3)\n",
    "    plt.plot(data_2, data_2_prob_density_function_lognorm, label='Probability density function (log normal): ' + label_2, color='r', linewidth=3)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.xlabel('Number of bids by tender')\n",
    "    plt.xlim(0, max_xlim)\n",
    "    plt.xticks(np.arange(0, max_xlim, step=max_xlim/bins))\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "    # Axis in percentage format\n",
    "    ax = fig.gca()\n",
    "    ax.set_yticklabels(['{:.2f}%'.format(x*100) for x in plt.gca().get_yticks()])\n",
    "\n",
    "    if plot_pdf:\n",
    "        name_file = dataset + '_Two_density_plots.pdf'\n",
    "        plt.savefig(name_file, format='pdf', dpi=1200, bbox_inches='tight')\n",
    "        print('Generated and saved file called ' + name_file)\n",
    "\n",
    "\n",
    "def print_description_processed_dataset(df):\n",
    "    ''' Print the most important information for the collusive dataset '''\n",
    "\n",
    "    # General \n",
    "    df_tenders = df.drop_duplicates(subset=['Tender', 'Number_bids'])\n",
    "    number_tenders = len(df['Tender'].unique())\n",
    "    print('')\n",
    "    print('------------------------------------')\n",
    "    print('Information of the collusive dataset')\n",
    "    print('Tenders: {0}'.format(number_tenders))\n",
    "    if 'Date' in df:\n",
    "        df_aux = df[df['Date'] > 0] # To avoid unavailable timestamps\n",
    "        minimum_date = datetime.datetime.fromtimestamp(df_aux['Date'].min())\n",
    "        maximum_date = datetime.datetime.fromtimestamp(df_aux['Date'].max())\n",
    "        print('Temporal range: {0}, {1}'.format(minimum_date, maximum_date))\n",
    "    number_bids = len(df)\n",
    "    print('Bids: {0}'.format(number_bids))\n",
    "    if 'Collusive_competitor_original' in df:\n",
    "        column_name = 'Collusive_competitor_original'\n",
    "    else:\n",
    "        column_name = 'Collusive_competitor'\n",
    "    number_collusive_bidders = len(df[df[column_name] == 1])\n",
    "    mean_number_bids = np.mean(df_tenders['Number_bids'])\n",
    "    print('Mean value of bidders per tender: {:,.2f}'.format(mean_number_bids)) \n",
    "    median_number_bids = np.median(df_tenders['Number_bids'])\n",
    "    print('Median value of bidders per tender: {:,.2f}'.format(median_number_bids)) \n",
    "    if 'Competitors' in df:\n",
    "        df_competitors = df.drop_duplicates(subset=['Competitors'])\n",
    "        number_competitors = len(df_competitors)\n",
    "        print('Competitors: {0}'.format(number_competitors))\n",
    "        number_winners = len(df_tenders['Competitors'].unique())\n",
    "        print('Winning competitors of tenders: {} ({:,.2f}%)'.format(number_winners, number_winners/number_competitors*100))\n",
    "\n",
    "    # Collusive vs competitive bids and tenders\n",
    "    number_collusive_tenders = len(df_tenders[df_tenders['Collusive_competitor'] == 1])\n",
    "    print('Collusive tenders: {} ({:,.2f}%)'.format(number_collusive_tenders, number_collusive_tenders/number_tenders*100))\n",
    "    number_competitive_tenders = len(df_tenders[df_tenders['Collusive_competitor'] == 0])\n",
    "    print('Competitive tenders: {} ({:,.2f}%)'.format(number_competitive_tenders, number_competitive_tenders/number_tenders*100)) \n",
    "    print('Collusive bids: {} ({:,.2f}%)'.format(number_collusive_bidders, number_collusive_bidders/number_bids*100))\n",
    "    number_competitive_bidders = len(df[df[column_name] == 0])\n",
    "    print('Competitive bids: {} ({:,.2f}%)'.format(number_competitive_bidders, number_competitive_bidders/number_bids*100))      \n",
    "    if 'Competitors' in df:\n",
    "        number_collusive_competitors = len(df_competitors[df_competitors['Collusive_competitor'] == 1])\n",
    "        print('Collusive competitors: {} ({:,.2f}%)'.format(number_collusive_competitors, number_collusive_competitors/number_competitors*100))\n",
    "        number_competitive_competitors = len(df_competitors[df_competitors['Collusive_competitor'] == 0])\n",
    "        print('Competitive competitors: {} ({:,.2f}%)'.format(number_competitive_competitors, number_competitive_competitors/number_competitors*100)) \n",
    "\n",
    "    # Number of tenders by received offers: 1-4, 5-10, >10\n",
    "    tenders_by_offer_group_1 = len(df_tenders[df_tenders['Number_bids'] <= 4])\n",
    "    print('Bids by tender: 1<=N<=4: {} ({:,.2f}%)'.format(tenders_by_offer_group_1, tenders_by_offer_group_1/number_tenders*100))\n",
    "    tenders_by_offers_group_2 = len(df_tenders[df_tenders['Number_bids'] <= 10]) - tenders_by_offer_group_1\n",
    "    print('Bids by tender: 5<=N<=10: {} ({:,.2f}%)'.format(tenders_by_offers_group_2, tenders_by_offers_group_2/number_tenders*100))\n",
    "    tenders_by_offers_group_3 = len(df_tenders[df_tenders['Number_bids'] > 10])\n",
    "    print('Bids by tender: 11<=N: {} ({:,.2f}%)'.format(tenders_by_offers_group_3, tenders_by_offers_group_3/number_tenders*100))\n",
    "\n",
    "    # Values of the winner's bid\n",
    "    df_winners = df[df['Winner'] == 1]\n",
    "    aggregated_bid_value = df_winners['Bid_value'].sum()\n",
    "    print('Aggregated tender price: {:,.0f}'.format(aggregated_bid_value))\n",
    "    aggregated_collusive_bid_value = df_winners[df_winners[column_name] == 1]['Bid_value'].sum()\n",
    "    print('Aggregated collusive tender price: {:,.0f} ({:,.2f}%)'.format(aggregated_collusive_bid_value, aggregated_collusive_bid_value/aggregated_bid_value*100))\n",
    "    aggregated_competitive_bid_value = df_winners[df_winners[column_name] == 0]['Bid_value'].sum()\n",
    "    print('Aggregated competitive tender price: {:,.0f} ({:,.2f}%)'.format(aggregated_competitive_bid_value, aggregated_competitive_bid_value/aggregated_bid_value*100))\n",
    "    mean_bid_value = np.mean(df_winners['Bid_value'])\n",
    "    print('Mean tender price: {:,.2f}'.format(mean_bid_value))\n",
    "    median_bid_value = np.median(df_winners['Bid_value'])\n",
    "    print('Median tender price: {:,.2f}'.format(median_bid_value))\n",
    "    print('------------------------------------')\n",
    "    print('')\n",
    "\n",
    "\n",
    "def get_dataset(dataset):\n",
    "    ''' Get the collusive dataset and their fields to use in the ML algorimths '''\n",
    "\n",
    "    predictors = defaultdict(list)\n",
    "\n",
    "    if dataset == 'brazilian_comprasnet':\n",
    "        df_collusion = pd.read_csv(db_collusion_brazilian_comprasnet, header=0)\n",
    "        df_collusion['Collusive_competitor'] = df_collusion['Collusive_competitor_original']\n",
    "        df_collusion.drop('material', inplace=True, axis=1)\n",
    "        predictors['all_setting'] = ['Tender', 'Bid_value', 'Pre_Tender', 'Difference_Bid_PTE', 'Date', 'Winner', 'especificidade', 'frequencia', 'ipgQuantidade', 'Number_bids']\n",
    "        predictors['all_setting+screens'] = predictors['all_setting'] + screens\n",
    "        predictors['common'] = ['Tender', 'Bid_value', 'Winner', 'Date', 'Number_bids']\n",
    "    elif dataset == 'brazilian':\n",
    "        df_collusion = pd.read_csv(db_collusion_brazilian, header=0)\n",
    "        predictors['all_setting'] = ['Tender', 'Bid_value', 'Pre-Tender Estimate (PTE)', 'Difference Bid/PTE', 'Site', 'Date', 'Brazilian State', 'Winner', 'Number_bids']\n",
    "        predictors['all_setting+screens'] = predictors['all_setting'] + screens\n",
    "        predictors['common'] = ['Tender', 'Bid_value', 'Winner', 'Date', 'Number_bids']\n",
    "    \n",
    "    elif dataset == 'switzerland_gr_sg':\n",
    "        df_collusion = pd.read_csv(db_collusion_switzerland_gr_sg, header=0)\n",
    "        predictors['all_setting'] = ['Tender', 'Bid_value', 'Contract_type', 'Date', 'Winner', 'Number_bids']\n",
    "        predictors['all_setting+screens'] = predictors['all_setting'] + screens\n",
    "        predictors['common'] = ['Tender', 'Bid_value', 'Winner', 'Date', 'Number_bids']\n",
    "    \n",
    "    elif dataset == 'switzerland_ticino':\n",
    "        df_collusion = pd.read_csv(db_collusion_switzerland_ticino, header=0)\n",
    "        predictors['all_setting'] = ['Tender', 'Bid_value', 'Consortium', 'Winner', 'Number_bids']\n",
    "        predictors['all_setting+screens'] = predictors['all_setting'] + screens\n",
    "        predictors['common'] = ['Tender', 'Bid_value', 'Winner', 'Number_bids']\n",
    "    \n",
    "    elif dataset == 'italian':\n",
    "        df_collusion = pd.read_csv(db_collusion_italian, header=0)\n",
    "        predictors['all_setting'] = ['Tender', 'Bid_value', 'Pre-Tender Estimate (PTE)', 'Difference Bid/PTE', 'Site', 'Capital', 'Legal_entity_type', 'Winner', 'Number_bids']\n",
    "        predictors['all_setting+screens'] = predictors['all_setting'] + screens\n",
    "        predictors['common'] = ['Tender', 'Bid_value', 'Winner', 'Number_bids']\n",
    "    \n",
    "    elif dataset == 'american':\n",
    "        df_collusion = pd.read_csv(db_collusion_american, header=0)\n",
    "        predictors['all_setting'] = ['Tender', 'Bid_value', 'Bid_value_without_inflation', 'Bid_value_inflation_raw_milk_price_adjusted_bid', 'Date', 'Winner', 'Number_bids']\n",
    "        predictors['all_setting+screens'] = predictors['all_setting'] + screens\n",
    "        predictors['common'] = ['Tender', 'Bid_value', 'Winner', 'Date', 'Number_bids']\n",
    "    \n",
    "    elif dataset == 'japan':\n",
    "        df_collusion = pd.read_csv(db_collusion_japan, header=0)\n",
    "        predictors['all_setting'] = ['Tender', 'Bid_value', 'Pre-Tender Estimate (PTE)', 'Difference Bid/PTE', 'Site', 'Date', 'Winner', 'Number_bids']\n",
    "        predictors['all_setting+screens'] = predictors['all_setting'] + screens\n",
    "        predictors['common'] = ['Tender', 'Bid_value', 'Winner', 'Date', 'Number_bids']\n",
    "    \n",
    "    elif dataset == 'all':\n",
    "        df_collusion = pd.read_csv(db_collusion_all, header=0)\n",
    "        predictors['common'] = ['Tender', 'Bid_value', 'Winner', 'Number_bids', 'Dataset']\n",
    "\n",
    "    predictors['common+screens'] = predictors['common'] + screens\n",
    "    \n",
    "    # Output fields of the datasets to the ML algorithms.\n",
    "    targets = ['Collusive_competitor'] \n",
    "\n",
    "    return df_collusion, predictors, targets \n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "    \n",
    "# The user selectes the dataset to analyse\n",
    "dataset = None\n",
    "while dataset == None:\n",
    "    number_input = input('Insert the number to analyse the dataset [brazilian_comprasnet (0),brazilian (1), american (2), italian (3), ' \\\n",
    "                     'switzerland_gr_sg (4), switzerland_ticino (5), japan (6), all datasets (7) or exit (E)]: ')\n",
    "    if number_input == 'E': sys.exit(0)\n",
    "    elif number_input == '0': dataset = 'brazilian_comprasnet'\n",
    "    elif number_input == '1': dataset = 'brazilian'\n",
    "    elif number_input == '2': dataset = 'american'\n",
    "    elif number_input == '3': dataset = 'italian'\n",
    "    elif number_input == '4': dataset = 'switzerland_gr_sg'\n",
    "    elif number_input == '5': dataset = 'switzerland_ticino'\n",
    "    elif number_input == '6': dataset = 'japan'       \n",
    "    elif number_input == '7': dataset = 'all'     \n",
    "\n",
    "# 1. Get the dataset processed ready to use with the ML algorithms\n",
    "df_collusion, predictors, targets = get_dataset(dataset)\n",
    "\n",
    "# 2. Print information of the processed datasets\n",
    "print_description_processed_dataset(df_collusion)\n",
    "\n",
    "# 3. Print list with the colluded tenders by bidder\n",
    "#calculate_colluded_tenders_by_bidder(df_collusion)\n",
    "\n",
    "# 4. Print Scatter Matrix\n",
    "df_scatter_matrix = shuffle_tenders(df_collusion)\n",
    "# Columns to plot\n",
    "columns_to_plot = ['Bid_value'] + screens + ['Collusive_competitor'] # Collusive_competitor is deleted at the end\n",
    "df_scatter_matrix = df_scatter_matrix[columns_to_plot]\n",
    "# Replace labels for colors to print the scatter matrix\n",
    "if 'Collusive_competitor_original' in df_scatter_matrix:\n",
    "    df_scatter_matrix['Collusive_competitor'] = df_scatter_matrix['Collusive_competitor_original']\n",
    "colors_legend = ['Green', 'Red']\n",
    "labels_legend = ['Competitive bid', 'Collusive bid']\n",
    "df_color_labels = df_scatter_matrix[['Collusive_competitor']].replace(0, colors_legend[0])\n",
    "df_color_labels = df_color_labels[['Collusive_competitor']].replace(1, colors_legend[1])\n",
    "list_color_labels = df_color_labels['Collusive_competitor'].values.tolist()\n",
    "df_scatter_matrix.drop(columns=['Collusive_competitor'], inplace=True)\n",
    "printScatterMatrix(df_scatter_matrix, list_color_labels, colors_legend, labels_legend, dataset)\n",
    "\n",
    "# 5. Boxplots of screen variables\n",
    "# Check with len of screens\n",
    "max_ylim_screens = [0.4, 1.8, 0.3, 12, 4, 4, 1]\n",
    "min_ylim_screens = [0, 0, 0, -12, -4, -4, 0]\n",
    "step_y_screens = [0.4/10, 1.8/10, 0.3/10, 24/10, 8/10, 8/10, 1/10]\n",
    "df_collusion_copy = df_collusion.copy()\n",
    "df_collusion_copy['Collusive_competitor'].replace(0, 'Comp.', inplace=True)\n",
    "df_collusion_copy['Collusive_competitor'].replace(1, 'Coll.', inplace=True)\n",
    "for index, screen_variable in enumerate(screens):\n",
    "    print_boxplot(df_collusion_copy, dataset, column_names=screen_variable, groupby='Collusive_competitor', min_ylim=min_ylim_screens[index], \n",
    "                  max_ylim=max_ylim_screens[index], step_y=step_y_screens[index], xlabel='Bids', percentage=True)\n",
    "\n",
    "# 6. Histogram or density plot of Number of Bids by Tender. Each plot for collusive tenders and honest tenders.\n",
    "\n",
    "df_hist = df_collusion\n",
    "if 'Collusive_competitor_original' in df_hist:\n",
    "    df_hist['Collusive_competitor'] = df_hist['Collusive_competitor_original']\n",
    "competitive_bids = df_hist[df_hist['Collusive_competitor'] == 0]['Number_bids']\n",
    "collusive_bids = df_hist[df_hist['Collusive_competitor'] != 0]['Number_bids']\n",
    "plotTwoHistograms(competitive_bids, collusive_bids, label_1='competitive bids', label_2='collusive bids', max_range=125, bins=25, max_xlim=125, density=True)  \n",
    "\n",
    "# 7. Execute algorithm comparison and print table comparison  \n",
    "algorithm_comparison(df_collusion, dataset, predictors, targets, ml_algorithms, train_size, repetitions, n_estimators, precision_recall, load_data, save_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763beb53-89e6-4c58-bc5c-2d13ce52ccad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
